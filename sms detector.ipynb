{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "import pandas as pd\n",
    "\n",
    "ham_df = pd.read_excel('ham.xlsx')\n",
    "spam_df = pd.read_excel('spam.xlsx')\n",
    "\n",
    "# Assume your data has a column 'text' for the SMS content\n",
    "# and you are creating a new column 'label' for ham/spam\n",
    "\n",
    "ham_df['label'] = 'ham'\n",
    "spam_df['label'] = 'spam'\n",
    "\n",
    "# Combine data\n",
    "fdata = pd.concat([ham_df, spam_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the Excel files\n",
    "ham_df = pd.read_excel('ham.xlsx', engine='openpyxl')\n",
    "spam_df = pd.read_excel('spam.xlsx', engine='openpyxl')\n",
    "\n",
    "# Assume your data has a column 'text' for the SMS content\n",
    "# and you are creating a new column 'label' for ham/spam\n",
    "ham_df['label'] = 'ham'\n",
    "spam_df['label'] = 'spam'\n",
    "\n",
    "# Combine the data into a single DataFrame\n",
    "fdata = pd.concat([ham_df, spam_df], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(fdata.head())\n",
    "\n",
    "# Optionally, save the combined data to a new Excel file\n",
    "fdata.to_excel('combined_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "fdata = shuffle(fdata)\n",
    "fdata = fdata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "fdata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata['message'] = fdata['message'].apply(lambda x: x.lower())\n",
    "fdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "\n",
    "fdata['message'] = fdata['message'].apply(punctuation_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "stop_words_to_lower = []\n",
    "stopwords = [\n",
    "    \"Waxaan\",\"wuxuu\",\"iyo\",\"ku\",\"oo\",\"aad\",\"aan\",\"een\",\"ee\",\"soo\",\"ka\",\"uu\",\"ay\",\"ey\",\"marka\",\"waxaa\",\"waxa\",\"wax\",\"in\",\"ah\",\"ayo\",\"mr\",\"u\",\"isu\",\"iyo\",\"waa\",\"ayaa\",\"mid\",\"isku\",\"taasi\",\"la\",\"Muxuu\",\"maxay\",\"inta\",\"uun\",\"uma\",\"sidii\",\"ugu\",\"mar\",\"kasoo\",\"si\",\"hor\",\"ma\",\"balse\",\"e\",\"waxayna\",\"inuu\",\"sii\",\"is\",\"miyuu\",\"U\",\"inay\",\"ayuu\",\"ke\",\"jira\",\"jirtey\",\"kale\",\"lagu\",\"laga\",\"kaliya\",\"jeer\",\"looga\",\"qaab\",\"cusub\",\"labada\",\"ayey\",\"ayay\",\"sida\",\"waayey\",\"manta\",\"maanta\",\"maalin\",\"maalinta\",\"mida\",\"shalay\",\"jirta\",\"xa\",\"doonaa\",\"dona\"]\n",
    "for element in range(len(stopwords)):\n",
    "    stop_words_to_lower.append(stopwords[element].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Initialize an empty list to store the processed messages\n",
    "corpus_message = []\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for i in range(len(fdata)):\n",
    "    # Extract and preprocess the text from the 'message' column\n",
    "    message = str(fdata['message'][i])\n",
    "    \n",
    "    # Keep dollar signs and numbers, remove other non-alphabetical characters\n",
    "    message = re.sub('[^a-zA-Z0-9$]', ' ', message)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    message = message.lower()\n",
    "    \n",
    "    # Split into words\n",
    "    message = message.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    message = [word for word in message if word not in stop_words_to_lower]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    message = ' '.join(message)\n",
    "    \n",
    "    # Append the processed message to the corpus list\n",
    "    corpus_message.append(message)\n",
    "\n",
    "# Display the first few processed messages to verify\n",
    "print(corpus_message[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "bow_cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "bow_cv_matrix = bow_cv.fit_transform(fdata['message'])\n",
    "bow_cv_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting into numpy array\n",
    "cv_matrix_bow = bow_cv_matrix.toarray()\n",
    "# get all unique words in the corpus\n",
    "# vocab = bow_cv.get_feature_names()........replace to out \n",
    "vocab = bow_cv.get_feature_names_out()\n",
    "\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix_bow, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set the n-gram range to 2,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2,2))\n",
    "bv_matrix = bv.fit_transform(fdata['message'])\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "\n",
    "# vocab = bow_cv.get_feature_names_out()\n",
    "vocab = bv.get_feature_names_out()\n",
    "\n",
    "# vocab = bv.get_feature_names()\n",
    "pd.DataFrame(bv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv = TfidfVectorizer(max_features=5000)\n",
    "features = cv.fit_transform(fdata['message'])\n",
    "\n",
    "vocab = cv.get_feature_names_out()\n",
    "print(\"Number of features:\", len(vocab))\n",
    "cv = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.5)\n",
    "X_new = cv.fit_transform(fdata['message'])\n",
    "\n",
    "# Check the new shape\n",
    "print(X_new.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'cv' is a TfidfVectorizer initialized with max_features=5000\n",
    "X = cv.fit_transform(fdata['message'])\n",
    "print(\"Shape of X:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming X is the output from TfidfVectorizer and is sparse\n",
    "X_dense = X.toarray()  # Convert the sparse matrix X to a dense matrix for use in DataFrame\n",
    "\n",
    "# Ensure the vectorizer is already fitted and get the feature names\n",
    "vocab = cv.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame using the dense array and feature names\n",
    "df = pd.DataFrame(np.round(X_dense, 2), columns=vocab)\n",
    "\n",
    "# Display the first few rows to confirm it's set up correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Since X is a sparse matrix, convert it to a dense format for use in a DataFrame\n",
    "X_dense = X.toarray()\n",
    "\n",
    "# Retrieve feature names from the TfidfVectorizer\n",
    "vocab = cv.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with the dense data and vocabulary as column headers\n",
    "df_tfidf = pd.DataFrame(np.round(X_dense, 2), columns=vocab)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(df_tfidf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the target\n",
    "y=pd.get_dummies(fdata['label'])\n",
    "y=y.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, fdata['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  naive_bayes object\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nb_model = MultinomialNB(alpha=0.8)\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on train data\n",
    "X_train_predict = nb_model.predict(X_train)\n",
    "# train_accuracy\n",
    "train_accuracy1 = accuracy_score(y_train, X_train_predict)\n",
    "\n",
    "# predict on test data\n",
    "X_test_predict1 = nb_model.predict(X_test)\n",
    "# test_accuracy\n",
    "test_accuracy1 = accuracy_score(y_test, X_test_predict1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score of training data : {round(train_accuracy1*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score of Testing data :{round(train_accuracy1*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, X_test_predict1)\n",
    "plot_confusion_matrix(cm, classes=['spam', 'ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prediction = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_accuracy = accuracy_score(X_train_prediction, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score of training data  :{round(training_data_accuracy*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prediction = model.predict(X_test)\n",
    "testing_data_accuracy = accuracy_score(X_test_prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score of testing data :{round(testing_data_accuracy*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(lr_model, open('logistic_regression_model.pkl', 'wb'))\n",
    "pickle.dump(nb_model, open('naive_bayes_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vectorizer\n",
    "import pickle\n",
    "\n",
    "filename = 'vectorizer.pkl'\n",
    "pickle.dump(cv, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Vectorize the text data\n",
    "cv = TfidfVectorizer(max_features=5000, max_df=0.5, min_df=5)\n",
    "X_tfidf = cv.fit_transform(fdata['message'])\n",
    "\n",
    "# Get the target labels\n",
    "y = pd.get_dummies(fdata['label'])\n",
    "y = y.iloc[:, 1].values  # This will select the second column if there are two classes\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "nb_model_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_model_report = classification_report(y_test, nb_predictions, target_names=['ham', 'spam'])\n",
    "\n",
    "print(f'Naive Bayes Model Accuracy: {nb_model_accuracy}')\n",
    "print(f'Naive Bayes Classification Report:\\n{nb_model_report}', )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the target labels\n",
    "y = pd.get_dummies(fdata['label'])\n",
    "y = y.iloc[:, 1].values  # This will select the second column if there are two classes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "lr_model_accuracy = accuracy_score(y_test, lr_predictions)\n",
    "lr_model_report = classification_report(y_test, lr_predictions, target_names=['ham', 'spam'])\n",
    "\n",
    "# Print Logistic Regression evaluation\n",
    "print(f'Logistic Regression Accuracy: {lr_model_accuracy}')\n",
    "print(f'Logistic Regression Classification Report:\\n{lr_model_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "pickle.dump(lr_model, open('logistic_regression_model.pkl', 'wb'))\n",
    "pickle.dump(nb_model, open('naive_bayes_model.pkl', 'wb'))\n",
    "\n",
    "# Save the TF-IDF Vectorizer\n",
    "pickle.dump(cv, open('vectorizer.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
